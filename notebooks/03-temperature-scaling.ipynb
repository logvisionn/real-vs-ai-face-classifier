{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c7b25c-c8d2-42d4-85e8-142ecbe6aebe",
   "metadata": {},
   "source": [
    "# 03 – Temperature Scaling on Validation Set\n",
    "\n",
    "This notebook finds the optimal **temperature** $T$ that, when dividing our model’s logits by $T$, minimizes the cross‐entropy loss on the validation set. In practice, this “flattens” the softmax so that extremely confident predictions (e.g. 99.9%) become more moderate (e.g. 90%).\n",
    "\n",
    "**Workflow:**\n",
    "1. Load the pretrained `FaceClassifier` (ResNet-18) from `models/best_model.pth`.  \n",
    "2. Build a validation `DataLoader` (same preprocessing as training).  \n",
    "3. Run a single pass over `val_loader` to collect all logits and labels.  \n",
    "4. Use `torch.optim.LBFGS` to optimize a single scalar $T$ to minimize  \n",
    "   $$\n",
    "   \\text{CrossEntropy}\\left(\\frac{\\text{logits}}{T},\\; \\text{labels}\\right)\n",
    "   $$\n",
    "5. Save the resulting $T$ to `models/best_temperature.pt`.  \n",
    "6. Visualize how dividing logits by $T$ changes softmax probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e560bc-f6d0-4ddc-b87d-f6182cf0ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✔️ Loaded FaceClassifier checkpoint from ../models/best_model_finetuned.pth\n",
      "Validation dataset has 57310 images,\n",
      "  organized into 2 classes: ['ai', 'real']\n"
     ]
    }
   ],
   "source": [
    "# Imports, Configuration, and Validation DataLoader ===\n",
    "\n",
    "import os, sys, warnings\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"]      = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# If this notebook lives in notebooks/, add ../src so we can import model.py\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import LBFGS\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from model import FaceClassifier   # now that src/ is on sys.path\n",
    "import numpy as np\n",
    "\n",
    "# ── 1) Device ────────────────────────────────────────────────────────────────\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ── 2) Path to your checkpoint and other constants ────────────────────────────\n",
    "MODEL_PATH = \"../models/best_model_finetuned.pth\"   # ← note the \"../\"\n",
    "IMG_SIZE    = 224\n",
    "MEAN        = [0.485, 0.456, 0.406]\n",
    "STD         = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ── 3) Load the trained FaceClassifier ────────────────────────────────────────\n",
    "model = FaceClassifier(backbone=\"resnet18\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(DEVICE).eval()\n",
    "print(\"✔️ Loaded FaceClassifier checkpoint from\", MODEL_PATH)\n",
    "\n",
    "# ── 4) Path to validation folder (one level up from notebooks/) ───────────────\n",
    "VAL_DIR = \"../data/processed/val\"    # ← note the \"../\"\n",
    "\n",
    "# ── 5) Define the exact same transforms used in training/notebook ─────────────\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "# ── 6) Create the dataset and loader ──────────────────────────────────────────\n",
    "val_dataset = ImageFolder(VAL_DIR, transform=val_transform)\n",
    "val_loader  = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Validation dataset has {len(val_dataset)} images,\")\n",
    "print(f\"  organized into {len(val_dataset.classes)} classes: {val_dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e299148a-57eb-45d4-bc09-56a0d30ee205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected all logits shape: torch.Size([57310, 2])\n",
      "Collected all labels shape: torch.Size([57310])\n"
     ]
    }
   ],
   "source": [
    "# Loop over val_loader to collect raw logits and ground-truth labels\n",
    "\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(DEVICE)       # [batch_size, 3, 224, 224]\n",
    "        logits = model(images)           # [batch_size, 2]\n",
    "        all_logits.append(logits.cpu())  # move to CPU\n",
    "        all_labels.append(labels)        # already on CPU\n",
    "\n",
    "# Concatenate into single tensors\n",
    "val_logits = torch.cat(all_logits, dim=0)  # shape: [N_val, 2]\n",
    "val_labels = torch.cat(all_labels, dim=0)  # shape: [N_val]\n",
    "\n",
    "print(\"Collected all logits shape:\", val_logits.shape)\n",
    "print(\"Collected all labels shape:\", val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbf2dcc-66fb-44cd-8bc9-885bbf320801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LBFGS temperature optimization…\n",
      "✔️ Done. Optimal temperature T = 1.7086546421051025\n"
     ]
    }
   ],
   "source": [
    "# Find optimal temperature T using LBFGS ===\n",
    "\n",
    "# 1) Initialize T as a learnable parameter, starting at 2.0\n",
    "T = torch.nn.Parameter(torch.ones(1) * 2.0, requires_grad=True)\n",
    "\n",
    "# 2) Create an LBFGS optimizer over [T]\n",
    "optimizer = LBFGS([T], lr=0.01, max_iter=50)\n",
    "\n",
    "def _loss():\n",
    "    optimizer.zero_grad()\n",
    "    scaled_logits = val_logits / T            # broadcast: shape [N_val, 2] ÷ [1]\n",
    "    loss = F.cross_entropy(scaled_logits, val_labels)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "print(\"Starting LBFGS temperature optimization…\")\n",
    "optimizer.step(_loss)\n",
    "print(\"✔️ Done. Optimal temperature T =\", float(T.detach()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51586c9f-3a2b-4692-82ee-af56c270d487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Saved optimal temperature to '../models/best_temperature.pt'\n"
     ]
    }
   ],
   "source": [
    "# Save T to 'models/best_temperature.pt' ===\n",
    "\n",
    "output_path = \"../models/best_temperature.pt\"\n",
    "torch.save(T.detach(), output_path)\n",
    "print(f\"✔️ Saved optimal temperature to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8511aa51-24de-41c1-bcba-a0bde4232025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample raw logits:\n",
      " [[5.  1. ]\n",
      " [1.2 2.4]]\n",
      "\n",
      "Raw softmax probabilities:\n",
      " [[0.98201376 0.01798621]\n",
      " [0.23147522 0.7685248 ]]\n",
      "\n",
      "Softmax probabilities after dividing by T = 1.71:\n",
      " [[0.912218   0.08778196]\n",
      " [0.33130094 0.6686991 ]]\n"
     ]
    }
   ],
   "source": [
    "# Show how dividing by T flattens softmax on example logits ===\n",
    "\n",
    "sample_logits = torch.tensor([\n",
    "    [5.0, 1.0],   # very confident “class 0”\n",
    "    [1.2, 2.4],   # moderately confident “class 1”\n",
    "])\n",
    "\n",
    "print(\"Sample raw logits:\\n\", sample_logits.numpy())\n",
    "\n",
    "with torch.no_grad():\n",
    "    raw_probs    = F.softmax(sample_logits, dim=1)\n",
    "    scaled_probs = F.softmax(sample_logits / T, dim=1)\n",
    "\n",
    "print(\"\\nRaw softmax probabilities:\\n\", raw_probs.numpy())\n",
    "print(f\"\\nSoftmax probabilities after dividing by T = {float(T):.2f}:\\n\", scaled_probs.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288654e9-7a8c-45b9-aa3b-6a7c38fb9348",
   "metadata": {},
   "source": [
    "## ✅ Summary: Temperature Scaling Calibration\n",
    "\n",
    "This notebook performs post-hoc calibration of the `FaceClassifier` using **temperature scaling**, a technique to reduce overconfidence in softmax outputs.\n",
    "\n",
    "- **Model checkpoint:** `models/best_model.pth`\n",
    "- **Validation set:** 57,310 images from `data/processed/val/`\n",
    "- **Optimization:** LBFGS minimized  \n",
    "  $\\text{CrossEntropy}(\\text{logits} / T,\\; \\text{labels})$\n",
    "- **Optimal temperature $T$ found:** **1.315**\n",
    "- **Effect:** Softmax probabilities became less extreme, improving confidence calibration.\n",
    "\n",
    "✅ Final temperature was saved to: `models/best_temperature.pt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24464d27-e67e-4294-84b8-9adb8ed64d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fnnd]",
   "language": "python",
   "name": "conda-env-fnnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
